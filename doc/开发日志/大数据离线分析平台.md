### 参考连接

橘云大数据平台设计的ppt,思路和架构都比较清晰
https://wenku.baidu.com/view/038124e0a36925c52cc58bd63186bceb19e8ed8d.html

vue -cli 官网

https://cli.vuejs.org/zh/guide/installation.html





项目名

基于Spark的大数据离线分析系统设计与实现



基于Spark的大数据离线分析平台设计与实现







> 解决传统手动ETL的痛点

### 其他待添加

- 支持选择 Pandas 或者 Spark 引擎。都是通过制定的json数据格式来处理

### 功能

- Hadoop 集群数据管理

  ETL 可以用于处理HDFS上的文件，ETL功能基于远程的Spark

- 基于Spark的可视化大数据ETL

  选择目标数据，然后可以预览数据（抽样等)

  后台读取成 SparkDataFrame 来操作。

  每个ETL由一个一个source , transform ,sink 组成

  

- 基于Hive的数据查询可视化

  > 在大量数据里查询一小部分,每次的结果不超过50k,或者1000行。结果缓存为csv文件
  
  添加查询，将查询缓存到缓冲区， 缓冲区合并查询

### 系统分层

数据层: 连接集群，HDFS 集群文件管理

开发层: 添加mr，spark程序任务到集群运行( 使用livy)

ETL层： 可视化数据ETL。 

可以 使用sql查询，转换数据。使用spark spark livy,hiveserver2
数据增量导入导出 

可视化图表层: 查询hbase,hive里的数据，并可视化结果

ETL功能包括但不限于:
- 提取指定列
- 使用sql查询
- 保存数据到其他
- 分组求和 
- 空值统计

### 技术栈

- 前后端数据库
  SpringBoot全家桶   + Vue.js + Mysql

  Django

- 大数据方面
  Hadoop + Spark + Hive + Livy

### 开发工具

- Eclipse or IDEA

  开发后台项目

- Visual Code

  开发前端项目

- Virtual Box 

  提供虚拟机，连接集群测试



### 系统设计

- 运行流程

- 架构图
- 实现思路
- 页面图

## Flow

### Configure

### ETL

执行ETL之前，先

## Concepts

### ETL 工程

一个处理的流程，有输入数据源，输出结果。

为方便测试，可以从原始数据里抽样出部分数据(预处理)，测试完成后，可以设置源为真实数据即可



### 执行ETL工程

显示一个对话框，进度条，执行完毕后，显示结果。可以设置远程的spark的配置

执行etl工程流程

1. 前端提交工程的id
2. 后端读取数据库或者使用前端提交过来的json，使用livy  programatic api来执行程序，将执行结果返回到前端
3. 

问题:

- 前端提交任务后，如何知道后端完成了，并显示结果。可以用ajax 不断查询，但是感觉不够好。



spark 根据json的内容执行，返回json, 结果包括,每个组件的执行结果。是否执行成功，

设计一个表，用来表示提交的了任务。可以对任务进行查询，更新。后端处理完后也直接将处理结果保存到

进度的保存

- Django 全局变量 或 session

- sqlite,mysql 数据库

  大材小用,还要建表，增查删改那些

- 本地文件

  每次都要读取

- redis

  感觉可行,但要加个redis..

  

  

## Features

### Hadoop Cluster

- 集群信息查看

- 连接hadoop集群
- 提供集群的文件管理，

### Data ETL

- 运行任务时可以指定spark的相关参数

- 数据转换(可视化界面)

  子集转换: 选择指定的列

  分组聚合: 对数据进行分组统计，选择分组的列，选择聚合的选项

  列处理

  删除行，删除某列包含xx的，某列==xx的,某列不等于xx的

  输出数据到指定目录

  转换操作

  | 操作                    |                            |                                 |
  | ----------------------- | -------------------------- | ------------------------------- |
  | 选择子集(select_col)    | 选择指定的列               |                                 |
  | 删除行(delete_row)      | 删除指定的行               | 所以空行，包含指定字符串的行,等 |
  | 日期格式化(format_date) | 格式化日期到指定形式       | 指定原格式，输出格式            |
  | 聚合操作(agg)           |                            |                                 |
  | 分组统计(groupby_col)   | 按指定列分组，统计非分组列 | 选择分组字段，添加聚合字段      |
  | 列操作(operate_col)     | 对某列进行操作，如取出空格 | 选择列                          |
  | 抽样输出(sample_row)    |                            |                                 |

  

- data statistic(计算)

  base 的 原数据统计功能

  | 操作                       |                  |                    |
  | -------------------------- | ---------------- | ------------------ |
  | 空值统计                   | 统计格列的空值数 | 指定用什么代表空值 |
  | 对某列进行求平均值，等操作 | 选择列，求列值， |                    |
  |                            |                  |                    |
  |                            |                  |                    |
  |                            |                  |                    |
  |                            |                  |                    |
  |                            |                  |                    |

  

  - 空值统计： 各列的空值，指定列的空值数，**指定什么样的值才是空值**
  - 基本统计:  

- task manager

### Data Query and Visulization

使用SQL

- query hive
- query hbase

- 结果导出

## Design

### Settings

配置界面

- 

```json
设置数据的格式
{
    //livy的地址
    "livyUrl":"",
    //hdfs的地址，地址+端口
    "hdfsUrl":"",
    
    //hiveserver2的地址
    "hiveUrl":""
    
    
}
```



### ETL

ETL界面

- 任务列表

- 任务编辑

## Datebases

数据库设计

- 配置表
- ETL 工程表



### 待添加features

- 前端输入hdfs路径时，支持选择路径
- 字段选择器

- 自动刷新任务监控

- hdfs管理优化

- 自定义spark运行参数





django restful 

http://127.0.0.1:8000/api/etl/projects/18/